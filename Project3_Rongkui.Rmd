---
title: "Project3"
author: "Rongkui Han"
date: "2/6/2020"
output:
  bookdown::pdf_document2:
    toc: FALSE
---

Make suggestions to policymakers to take certain measures by discovering variables that caused the reduction or increase of traffic fatalities.    

```{r}
ggplot(data, aes(x = drinkage, y = fr)) +
  geom_point()
```

## Propensity Score matching
```{r}
library(MatchIt)
library(dplyr)
library(ggplot2)
library(lme4)
```

```{r}
library(AER)
data("Fatalities")
data = Fatalities

#Impute values for missing CA data
data[28,15] = as.factor("no")
data[28,16] = as.factor("no")


#head(data)
#dim(data)
data['fr'] = data$fatal/data$pop*10000
```

### Propensity score estimation    

We estimate the propensity score by running a logit model (probit also works) where the outcome variable is a binary variable indicating treatment status. What covariates should you include? For the matching to give you a causal estimate in the end, you need to include any covariate that is related to **both the treatment assignment and potential outcomes**. I choose just a few covariates below—they are unlikely to capture all covariates that should be included. You’ll be asked to come up with a potentially better model on your own later.

```{r}
data$jail = ifelse(data$jail == 'yes', 1, 0)
m_ps = glm(jail ~ year + spirits + unemp + income + beertax + baptist + mormon + drinkage + dry + youngdrivers + miles + breath + service + pop + gsp, family = binomial(), data = data)
summary(m_ps)
```

```{r}
prs_df = data.frame(pr_score = predict(m_ps, type = "response"),
                    jail = m_ps$model$jail)
#head(prs_df)
#dim(prs_df)
```

#### Examining the region of common support

After estimating the propensity score, it is useful to plot histograms of the estimated propensity scores by treatment status:    

```{r}
labs = paste("Actual jail law implemented:", c("Yes", "No"))
prs_df %>%
  mutate(jail = ifelse(jail == 1, labs[1], labs[2])) %>%
  ggplot(aes(x = pr_score)) +
  geom_histogram(color = "white") +
  facet_wrap(~jail) +
  xlab("Probability of implementing mandatory jail sentence") +
  theme_bw()
```

## Executing a matching algorithm     

A simple method for estimating the treatment effect of Catholic schooling is to restrict the sample to observations within the region of common support, and then to divide the sample within the region of common support into 5 quintiles, based on the estimated propensity score. Within each of these 5 quintiles, we can then estimate the mean difference in student achievement by treatment status. Rubin and others have argued that this is sufficient to eliminate 95% of the bias due to confounding of treatment status with a covariate.

However, most matching algorithms adopt slightly more complex methods. The method we use below is to find pairs of observations that have very similar propensity scores, but that differ in their treatment status. We use the package MatchIt for this. This package estimates the propensity score in the background and then matches observations based on the method of choice (“nearest” in this case).

```{r}
data_cov = c("year", "spirits" , "unemp" , "income" , "beertax" , "baptist" , "mormon" , "drinkage" , "dry" , "youngdrivers" , "miles" , "breath" , "service" , "pop" , "gsp")

mod_match = matchit(jail ~ year + spirits + unemp + income + beertax + baptist + mormon + drinkage + dry + youngdrivers + miles + breath + service + pop + gsp, method = "nearest", data = data)

dta_m = match.data(mod_match)
#dim(dta_m)
```

## Examining covariate balance in the matched sample    

We’ll do three things to assess covariate balance in the matched sample:

- visual inspection
- t-tests of difference-in-means
- computation of the average absolute standardized difference (“standardized imbalance”)



```{r}
fn_bal <- function(dta, variable) {
  dta$variable <- dta[, variable]
  dta$jail <- as.factor(dta$jail)
  support <- c(min(dta$variable), max(dta$variable))
  ggplot(dta, aes(x = distance, y = variable, color = jail)) +
    geom_point(alpha = 0.2, size = 1.3) +
    geom_smooth(method = "loess", se = F) +
    xlab("Propensity score") +
    ylab(variable) +
    theme_bw() +
    ylim(support)
}

library(gridExtra)
grid.arrange(
   #fn_bal(dta_m, "year"),
   fn_bal(dta_m, "spirits"),
   fn_bal(dta_m, "unemp"),
   fn_bal(dta_m, "income"), #+ theme(legend.position = "none"),
   fn_bal(dta_m, "beertax"),
   #fn_bal(dta_m, "baptist"),
   #fn_bal(dta_m, "mormon"),
   fn_bal(dta_m, "drinkage"),
   fn_bal(dta_m, "dry"),
   fn_bal(dta_m, "youngdrivers"),
   fn_bal(dta_m, "miles"),
   #fn_bal(dta_m, "breath"),
   #fn_bal(dta_m, "service"),
   fn_bal(dta_m, "pop"),
   fn_bal(dta_m, "gsp"),
   nrow = 4#, widths = c(1, 0.8)
)
```

#### Difference-in-means
The means below indicate that we have attained a high degree of balance on the five covariates included in the model.

```{r}
dta_m_mean = dta_m %>%
  group_by(jail) %>%
  summarise_all(funs(mean))
dta_m_mean = dta_m_mean[,c('jail', data_cov)]
dta_m_mean
```

You can test this more formally using t-tests. Ideally, we should not be able to reject the null hypothesis of no mean difference for each covariate:   


Not working: 
```{r, eval = FALSE}
lapply(data_cov, function(v) {
    t.test(dta_m[,v] ~ dta_m$jail)
})
```

## Estimating treatment effects
Estimating the treatment effect is simple once we have a matched sample that we are happy with. We can use a t-test:

```{r}
with(dta_m, t.test(fr ~ jail))
```

```{r}
summary(data$fr)
```


Or OLS:  
```{r}
lm_treat1 <- lm(fr ~ jail, data = dta_m)
summary(lm_treat1)
confint(lm_treat1)

lm_orig = lm(fr ~ jail, data = data)
summary(lm_orig)
confint(lm_orig)

sum(dta_m$jail == 1) #94
sum(dta_m$jail == 0) #94

sum(data$jail == 1)
sum(data$jail == 0)

mean(dta_m$fr[dta_m$jail == 1]) - mean(dta_m$fr[dta_m$jail == 0])

mean(dta_m$fr[dta_m$jail == 0])
mean(dta_m$fr[dta_m$jail == 1])


mean(data$fr[data$jail == 1]) - mean(data$fr[data$jail == 0])
```


```{r}
#What about if we inlcude state in the linear regression model

#Fixed
fit1 = lm(fr ~ jail + state, data = dta_m)
summary(fit1)

#Random
fit2 <- lmer(fr ~ jail + (1 | state), data = dta_m)
summary(fit2)

```


***

### Team ID: Team 6

#### NAME: Connor Rosenberg
#### NAME: Rongkui Han
#### NAME: Yuqing Yang
#### NAME: Nassim Ali-Chaouche

***

## 1.0 Introduction


#### 1.1 Background

The "Fatalities" data set contains data pertinent to US traffic fatalities from the years 1982 to 1988. The data set includes panel data for 48 states (data from Alaska and Hawaii are not included). The variables involved in the study include income per capita, a beer tax, a mandatory jail sentence for an initial drunk driving conviction, population, and the number of vehicle fatalities. The observations were recorded for each state annually. In total, there are 336 observations recorded for 34 distinct variables. The data was obtained from sources such as the US Department of Transportation Fatal Accident Reporting System and the US Bureau of Labor Statistics. 

Due to the observational nature of the data, obtaining causal effects may pose to be a challenge. In this report, we will attempt to discover if there is a causal relationship between having a mandatory jail sentence for an initial drunk driving conviction and the number of traffic fatalities. In particular, we will study if a mandatory jail sentence is related to having a lower number of traffic fatalities. 

#### 1.2 Questions of Interest

         

## 2.0 Analysis Plan

The primary objective of this analysis is to educate State legislators on whether a mandatory jail sentence is a proper current and will result in lower automobile fatality rates. From data collected by the National Highway Traffic Safety Administration's Fatal Accident Reporting System (FARS), we plan to conduct a propensity score analysis to isolate the average causal effect of required jail time on automobile fatality rates.

#### 2.1 Population and study design
check if time compente is independtneneed to check that time is independent within each state. 

remove states that changed policies and do a linear regression w/ time to see if it effects the fatality rate.

#### 2.2 Descriptive Analysis   
spagetti plot

#### 2.3 Propensity Score Analysis

##### 2.3.1 Propensity Score Estimation

We will estimate the propensity score through a logistical model. The outcome variable is a binary variable indicating treatment status, wheater or not a State has a mandatory jail sentence. And the output of this model is the probability that a State has a mandatory jail sentence given our data of covariates. The logistical model we used to estimate the propensity score is as follows:

##### 2.3.2 Maching

To match the samples with mandatory jail sentences to samples without, we will divide the data contained within the region of common support into 5 quintiles, based on the estimated propensity score. Within each of these 5 quintiles, we can then estimate the mean difference in fatality rate by jail status. Rubin and others have argued that this is sufficient to eliminate 95% of the bias due to the confounding of treatment status with a covariate.

To match observations with mandatory jail sentences to observations without, we will use the nearest neighbor matching algorithm based upon our generated propensity score.  

##### 2.3.3 Examining covariate balance in the matched sample    

We must assess the covariate balance in our matched sample to ensure our assumptions for a propensity score are met. We will perform a visual inspection through covariate balance plots and perform several t-tests for difference-in-means.

##### 2.3.4 Estimate Treatment Effect

To estimate the effect of mandatory jail sentences on a State's fatality rate, we will fit the following linear regression relating the binary treatment variable to Fatality Rate.





#### 2.4 Model Diagnostics


## 3.0 Results


#### 3.1 Descriptive Analysis     

#### 3.2 Propensity Score Analysis


##### 3.2.1 Propensity Score Estimation

##### 3.2.2 Maching

##### 3.2.3 Examining covariate balance in the matched sample    

##### 3.2.4 Estimate Treatment Effect


#### 3.3 Model Diagnostics


#### 3.4 Causal Effects


## 4.0 Discussion


## 5.0 Reference

